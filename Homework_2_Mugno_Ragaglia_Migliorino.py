# -*- coding: utf-8 -*-
"""Homework_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bvK2Fc0NS3FJT9O5PB1JKczKL43NDCCm
"""

#Imports utilities
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import urllib

#Download our reviews from google drive
urllib.request.urlretrieve('https://drive.google.com/uc?id=1Bc-Y2vWWWDVb7vs-38LdcJKi02z2VIQA&export=download', 'reviews.txt')

#Load reviews to string
reviews_path = "reviews.txt"
with open(reviews_path, "r") as f:
    reviews = f.read()

#Split the reviews accroding to \n
reviews = reviews.split("\n")

print([i for i in reviews if i.startswith('\t')])

#Remove strings without label
filtered_reviews = [r for r in reviews if not r.startswith('\t')]

#Checking all strings strating with '\t', so the one without label
print([i for i in filtered_reviews if i.startswith('\t')])

#Checking all strings strating with '1\t ', so the one without reviews
print([i for i in filtered_reviews if i.startswith('1\t ')])

#Checking all strings strating with '0\t ', so the one without reviews
print([i for i in filtered_reviews if i.startswith('0\t ')])

#Removing strings without reviews
filtered_reviews = [r for r in filtered_reviews if not r.startswith('1\t ')]

filtered_reviews = [r for r in filtered_reviews if not r.startswith('0\t ')]

#Check if they are removed
print([i for i in filtered_reviews if i.startswith('1\t ')])

#Check if they are removed
print([i for i in filtered_reviews if i.startswith('0\t ')])

#Deleting last empty string
filtered_reviews = filtered_reviews[:-1]

labels = list ()
description = list ()

#Creating two separate lists, one for description and one for label
for i in range(len(filtered_reviews)):
  string = filtered_reviews[i].split('\t')
  labels.append(string[0])
  description.append(string[1])

#Deleting the title
labels = labels[1:]

#Deleting the title
description = description[1:]

#Splitting all the words in each string
description = [[w for w in d.split(" ") if len(w)>0] for d in description]

#Build vocabulary
words = list(set([w for d in description for w in d]))
vocab = {words[i]: i+1 for i in range(len(words))} # we reserve i=0 for pad sequences

#Convert each description into indixes
description = [[vocab[w] for w in d] for d in description]

seq_len = 200 #numbers of words for each senteces

description = [d[:seq_len] for d in description]

#Pad reviews shorter than seq_len (adding 0 on the string)
description = [[0]*(seq_len - len(d)) + d for d in description]

#Change string '1' to 1 and string '0' to 0
labels = [0 if labels[i]=='0' else 1 for i in range(len(labels))]

list_data = [labels,description]

set_zero = list()
set_uno = list()
test = 0

#Dividing in two sets (one for with labels 1 and one with labels 0)
for i in range(len(filtered_reviews)-1):
  test = [row[i] for row in list_data]
  if test[0] == 0:
    set_zero.append(test)
  else:
    set_uno.append(test)

#Shuffling, creating sets and transforming them in tensors
import random
random.Random(100).shuffle(set_zero)
random.Random(100).shuffle(set_uno)

test_data = set_zero[:20] + set_uno[:20]

validation_data = set_zero[20:30] + set_uno[20:30]

train_data_unbalanced = set_zero[30:] + set_uno[30:]

train_data = set_zero[30:] + set_uno[30:216]

for i in range(len(train_data)):
  test = [row[0] for row in train_data]
  train_data_lab = torch.LongTensor(test)

for i in range(len(train_data)):
  test = [row[1] for row in train_data]
  train_data_desc = torch.LongTensor(test)

for i in range(len(train_data_unbalanced)):
  test = [row[0] for row in train_data_unbalanced]
  train_data_lab_unbalanced = torch.LongTensor(test)

for i in range(len(train_data_unbalanced)):
  test = [row[1] for row in train_data_unbalanced]
  train_data_desc_unbalanced = torch.LongTensor(test)

for i in range(len(validation_data)):
  test = [row[0] for row in validation_data]
  validation_data_lab = torch.LongTensor(test)

for i in range(len(validation_data)):
  test = [row[1] for row in validation_data]
  validation_data_desc = torch.LongTensor(test)

for i in range(len(test_data)):
  test = [row[0] for row in test_data]
  test_data_lab = torch.LongTensor(test)

for i in range(len(test_data)):
  test = [row[1] for row in test_data]
  test_data_desc = torch.LongTensor(test)

#Create tensor datasets
train_dataset = TensorDataset(train_data_desc, train_data_lab)
train_dataset_unbalanced = TensorDataset(train_data_desc_unbalanced, train_data_lab_unbalanced)
validation_dataset = TensorDataset(validation_data_desc, validation_data_lab)
test_dataset = TensorDataset(test_data_desc, test_data_lab)

batch_size = 8
batch_size_unbalanced = 64

#Create loaders
loaders = {"train": DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  drop_last=True),
           "val":   DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=False),
           "test":  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)}

loaders_unbalanced = {"train_unbalanced": DataLoader(train_dataset_unbalanced, batch_size=batch_size_unbalanced, shuffle=True,  drop_last=True),
           "val":   DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, drop_last=False),
           "test":  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)}

#Define model
class Model(nn.Module):
    
    def __init__(self, num_embed, embed_size, rnn_size):
        #params: 
        #num_embed: the number of the input vocabulary
        #embed_size: the size of the feature embedding
        #rnn_size: the number of neurons in the recurrent layer

        #Call parent constructor
        super().__init__()
        #Store values
        self.rnn_size = rnn_size
        #Define modules
        self.embedding = nn.Embedding(len(vocab)+1, embed_size)
        self.rnn = nn.Dropout()
        self.rnn = nn.RNNCell(embed_size, rnn_size) #RNNCell represents only a single time step
        self.output = nn.Linear(rnn_size, 2)
        
    def forward(self, x):
        #Embed data
        x = self.embedding(x)
        #Initialize state
        h = torch.zeros(x.shape[0], self.rnn_size).to(x.device.type) # the state of the cell
        
        #Input is: B x T x F
        #Process each time step
        for t in range(x.shape[1]):
            #Input at time t
            x_t = x[:,t,:]
            #Forward RNN and get new state
            h = self.rnn(x_t, h)
        #Classify final state
        x = self.output(h)
        return x

#Setup device
dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Model parameters
embed_size = 1024
rnn_size = 256

#Create model
model = Model(len(vocab)+1, embed_size, rnn_size).to(dev)

#Test model output
model.eval()
test_input = train_dataset[0][0].unsqueeze(0).to(dev)
print("Model output size:", model(test_input).size())

#Compute the percentages of each class
w1=2748/2964
w0=216/2964

weights = [1-w0, 1-w1]
#Important: Convert Weights To Float Tensor
class_weights = torch.FloatTensor(weights).cuda()
#Define a loss for unbalanced train data
criterion_unbalanced = nn.CrossEntropyLoss(weight=class_weights)
#Create the list of learning rates
learning_rate = [0.01, 0.001, 0.05, 0.005]
#Define a loss for balanced train data
criterion = nn.CrossEntropyLoss()

#Start traning
x = 0
x_unbalanced = 0
epoch_count = 0
epoch_count_unbalanced = 0
counter = 0

from tqdm import tqdm

for i in range(len(learning_rate)):
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate[i], weight_decay=5e-4)
  for epoch in range(100):
    counter = counter + 1
    #Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train': 0, 'val': 0, 'test': 0}
    #Process each split
    for split in loaders:
        #Set network mode
        if split == "train":
          model.train()
          torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        #Process all data in split
        for input, target in tqdm(loaders[split]):
            #Move to device
            input = input.to(dev)
            target = target.to(dev)
            #Reset gradients
            optimizer.zero_grad()
            #Forward
            output = model(input)
            loss = criterion(output, target)
            #Update loss sum
            epoch_loss_sum[split] += loss.item()
            epoch_loss_cnt[split] += 1
            #Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            #Update accuracy sum
            epoch_accuracy_sum[split] += accuracy
            epoch_accuracy_cnt[split] += 1
            #Backward and optimize
            if split == "train":
              loss.backward()
              optimizer.step()
    
    #Compute epoch loss/accuracy
    epoch_loss = {split: epoch_loss_sum[split]/len(loaders[split]) for split in ["train", "val", "test"]}
    epoch_accuracy = {split: epoch_accuracy_sum[split]/len(loaders[split]) for split in ["train", "val", "test"]}
    #Compute average epoch loss/accuracy
    avg_train_loss = epoch_loss_sum["train"]/epoch_loss_cnt["train"]
    avg_train_accuracy = epoch_accuracy_sum["train"]/epoch_accuracy_cnt["train"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]


    epoch = epoch+1
    if avg_val_accuracy >= x:
      x = avg_val_accuracy
      epoch_count = epoch

    if learning_rate[i] == 0.01:
      lr1 = learning_rate[i]
      x1 = x
      final_epoch1 = epoch_count
    elif learning_rate[i] == 0.001:
      lr2 = learning_rate[i]
      x2 = x
      final_epoch2 = epoch_count
    elif learning_rate[i] == 0.05:
      lr3 = learning_rate[i]
      x3 = x
      final_epoch3 = epoch_count
    elif learning_rate[i] == 0.005:
      lr4 = learning_rate[i]
      x4 = x
      final_epoch4 = epoch_count

    print(f"Epoch: {epoch}, TrL={avg_train_loss:.4f}, TrA={avg_train_accuracy:.4f},",
                            f"VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ",
                            f"TeL={avg_test_loss:.4f}, TeA={avg_test_accuracy:.4f}")
    print(f"Counter: {counter}")

    epoch = 0
    x_unbalanced = 0

  for epoch in range(100):
    counter = counter + 1
    #Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    #Process each split
    for split_unbalanced in loaders_unbalanced:
        #Set network mode
        if split_unbalanced == "train_unbalanced":
          model.train()
          torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        #Process all data in split
        for input, target in tqdm(loaders_unbalanced[split_unbalanced]):
            #Move to device
            input = input.to(dev)
            target = target.to(dev)
            #Reset gradients
            optimizer.zero_grad()
            #Forward
            output = model(input)
            loss_unbalanced = criterion_unbalanced(output, target)
            #Update loss sum
            epoch_loss_sum[split_unbalanced] += loss.item()
            epoch_loss_cnt[split_unbalanced] += 1
            #Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            #Update accuracy sum
            epoch_accuracy_sum[split_unbalanced] += accuracy
            epoch_accuracy_cnt[split_unbalanced] += 1
            #Backward and optimize
            if split_unbalanced == "train_unbalanced":
              loss_unbalanced.backward()
              optimizer.step()

    #Compute epoch loss/accuracy
    epoch_loss = {split_unbalanced: epoch_loss_sum[split_unbalanced]/len(loaders_unbalanced[split_unbalanced]) for split_unbalanced in ["train_unbalanced", "val", "test"]}
    epoch_accuracy = {split_unbalanced: epoch_accuracy_sum[split_unbalanced]/len(loaders_unbalanced[split_unbalanced]) for split_unbalanced in ["train_unbalanced", "val", "test"]}
    #Compute average epoch loss/accuracy
    avg_train_loss_unbalanced = epoch_loss_sum["train_unbalanced"]/epoch_loss_cnt["train_unbalanced"]
    avg_train_accuracy_unbalanced = epoch_accuracy_sum["train_unbalanced"]/epoch_accuracy_cnt["train_unbalanced"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]

    epoch = epoch+1
    if avg_val_accuracy >= x_unbalanced:
      x_unbalanced = avg_val_accuracy
      epoch_count_unbalanced = epoch
      
    if learning_rate[i] == 0.01:
      lr1_unb = learning_rate[i]
      x1_unb = x_unbalanced
      final_epoch_unb1 = epoch_count_unbalanced
    elif learning_rate[i] == 0.001:
      lr2_unb = learning_rate[i]
      x2_unb = x_unbalanced
      final_epoch_unb2 = epoch_count_unbalanced
    elif learning_rate[i] == 0.05:
      lr3_unb = learning_rate[i]
      x3_unb = x_unbalanced
      final_epoch_unb3 = epoch_count_unbalanced
    elif learning_rate[i] == 0.005:
      lr4_unb = learning_rate[i]
      x4_unb = x_unbalanced
      final_epoch_unb4 = epoch_count_unbalanced

    print(f"Epoch: {epoch}, TrL_unbalanced={avg_train_loss_unbalanced:.4f}, TrA_unbalanced={avg_train_accuracy_unbalanced:.4f},",
                            f"VL_unbalanced={avg_val_loss:.4f}, VA_unbalanced={avg_val_accuracy:.4f}, ",
                            f"TeL_unbalanced={avg_test_loss:.4f}, TeA_unbalanced={avg_test_accuracy:.4f}")
    print(f"Counter: {counter}")
    
print(f"Max result: Epoch: {final_epoch1}, Max_Val_Acc: {x1}, lr: {lr1}")
print(f"Max result: Epoch: {final_epoch2}, Max_Val_Acc: {x2}, lr: {lr2}")
print(f"Max result: Epoch: {final_epoch3}, Max_Val_Acc: {x3}, lr: {lr3}")
print(f"Max result: Epoch: {final_epoch4}, Max_Val_Acc: {x4}, lr: {lr4}")
print(f"Max result: Epoch: {final_epoch_unb1}, Max_Val_Acc: {x1_unb}, lr_unbalanced: {lr1_unb}")
print(f"Max result: Epoch: {final_epoch_unb2}, Max_Val_Acc: {x2_unb}, lr_unbalanced: {lr2_unb}")
print(f"Max result: Epoch: {final_epoch_unb3}, Max_Val_Acc: {x3_unb}, lr_unbalanced: {lr3_unb}")
print(f"Max result: Epoch: {final_epoch_unb4}, Max_Val_Acc: {x4_unb}, lr_unbalanced: {lr4_unb}")

#Best Model, lr = 0.005 unbalanced train
#Start training
from tqdm import tqdm
optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, weight_decay=5e-4)
for epoch in range(100):
    #Initialize accumulators for computing average loss/accuracy
    epoch_loss_sum = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_loss_cnt = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_accuracy_sum = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    epoch_accuracy_cnt = {'train_unbalanced': 0, 'val': 0, 'test': 0}
    #Process each split
    for split in ["train_unbalanced", "val", "test"]:
        #Set network mode
        if split == "train_unbalanced":
            model.train()
            torch.set_grad_enabled(True)
        else:
            model.eval()
            torch.set_grad_enabled(False)
        #Process all data in split
        for input, target in tqdm(loaders_unbalanced[split]):
            #Move to device
            input = input.to(dev)
            target = target.to(dev)
            #Reset gradients
            optimizer.zero_grad()
            #Forward
            output = model(input)
            loss = criterion_unbalanced(output, target)
            #Update loss sum
            epoch_loss_sum[split] += loss.item()
            epoch_loss_cnt[split] += 1
            #Compute accuracy
            _,pred = output.max(1)
            correct = pred.eq(target).sum().item()
            accuracy = correct/input.size(0)
            #Update accuracy sum
            epoch_accuracy_sum[split] += accuracy
            epoch_accuracy_cnt[split] += 1
            #Backward and optimize
            if split == "train_unbalanced":
                loss.backward()
                optimizer.step()
    #Compute average epoch loss/accuracy
    avg_train_loss = epoch_loss_sum["train_unbalanced"]/epoch_loss_cnt["train_unbalanced"]
    avg_train_accuracy = epoch_accuracy_sum["train_unbalanced"]/epoch_accuracy_cnt["train_unbalanced"]
    avg_val_loss = epoch_loss_sum["val"]/epoch_loss_cnt["val"]
    avg_val_accuracy = epoch_accuracy_sum["val"]/epoch_accuracy_cnt["val"]
    avg_test_loss = epoch_loss_sum["test"]/epoch_loss_cnt["test"]
    avg_test_accuracy = epoch_accuracy_sum["test"]/epoch_accuracy_cnt["test"]
    print(f"Epoch: {epoch+1}, TrL={avg_train_loss:.4f}, TrA={avg_train_accuracy:.4f},",
                            f"VL={avg_val_loss:.4f}, VA={avg_val_accuracy:.4f}, ",
                            f"TeL={avg_test_loss:.4f}, TeA={avg_test_accuracy:.4f}")

#Test review
review = "I like this product"
#review = "Honestly I was not happy for the service"

review = [vocab[w] for w in review.split(" ")]
review = [0]*(seq_len - len(review)) + review
review = torch.LongTensor(review).unsqueeze(0).to(dev)
#Process review
model.eval()
output = model(review).cpu()
probabilities = F.softmax(output,1)
_,pred = output.max(1)
pred = pred[0]
print(("positive" if pred == 1 else "negative") + f" ({100*probabilities[0,pred]:.1f}%)")